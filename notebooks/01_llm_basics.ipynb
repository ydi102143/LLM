{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM基礎実習: 最初のLLM体験\n",
        "\n",
        "このノートブックでは、LLMの基本的な使い方を学びます。\n",
        "\n",
        "## 学習目標\n",
        "- 簡単なLLMモデルを読み込んで使用する\n",
        "- プロンプトエンジニアリングの基礎を理解する\n",
        "- テキスト生成の基本的な仕組みを体験する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 簡単なLLMモデルの読み込み\n",
        "\n",
        "まずは、比較的軽量なGPT-2モデルを使ってみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# モデルとトークナイザーの読み込み\n",
        "model_name = \"gpt2\"  # 軽量なGPT-2モデル\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# パディングトークンを設定\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 基本的なテキスト生成\n",
        "\n",
        "シンプルなプロンプトからテキストを生成してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    テキストを生成する関数\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): 入力プロンプト\n",
        "        max_length (int): 最大生成長\n",
        "        temperature (float): 生成のランダム性（0.0-1.0）\n",
        "    \"\"\"\n",
        "    # テキストをトークン化\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # テキスト生成\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # 生成されたテキストをデコード\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# テスト用のプロンプト\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Once upon a time, there was a\",\n",
        "    \"In the world of machine learning,\"\n",
        "]\n",
        "\n",
        "print(\"=== 基本的なテキスト生成 ===\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- プロンプト {i} ---\")\n",
        "    print(f\"入力: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=80)\n",
        "    print(f\"出力: {generated}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
