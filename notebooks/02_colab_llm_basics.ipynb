{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMåŸºç¤å®Ÿç¿’: Google Colabç‰ˆ\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Google Colabã§LLMã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’å­¦ã¶ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚\n",
        "\n",
        "## å­¦ç¿’ç›®æ¨™\n",
        "- Google Colabã§LLMç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹\n",
        "- ç°¡å˜ãªLLMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ä½¿ç”¨ã™ã‚‹\n",
        "- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸºç¤ã‚’ç†è§£ã™ã‚‹\n",
        "- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã‚’ä½“é¨“ã™ã‚‹\n",
        "\n",
        "## æ³¨æ„äº‹é …\n",
        "- ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯Google Colabã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™\n",
        "- GPUã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  > ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ > GPU ã‚’é¸æŠã—ã¦ãã ã•ã„\n",
        "- ç„¡æ–™ç‰ˆã§ã¯åˆ¶é™ãŒã‚ã‚Šã¾ã™ãŒã€åŸºæœ¬çš„ãªå­¦ç¿’ã«ã¯ååˆ†ã§ã™\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "\n",
        "ã¾ãšã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets accelerate evaluate scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆå¯è¦–åŒ–ç”¨ï¼‰\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç°¡å˜ãªLLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
        "\n",
        "ã¾ãšã¯ã€æ¯”è¼ƒçš„è»½é‡ãªGPT-2ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚Colabã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
        "model_name = \"gpt2\"  # è»½é‡ãªGPT-2ãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "print(f\"ğŸ”„ {model_name} ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "print(\"åˆå›å®Ÿè¡Œæ™‚ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
        "print(f\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}\")\n",
        "print(f\"ğŸ”¤ èªå½™æ•°: {len(tokenizer)}\")\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "\n",
        "ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "        max_length (int): æœ€å¤§ç”Ÿæˆé•·\n",
        "        temperature (float): ç”Ÿæˆã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼ˆ0.0-1.0ï¼‰\n",
        "        top_p (float): æ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "    \"\"\"\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Once upon a time, there was a\",\n",
        "    \"In the world of machine learning,\",\n",
        "    \"The secret to success is\",\n",
        "    \"Climate change is one of the most\"\n",
        "]\n",
        "\n",
        "print(\"=== åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i} ---\")\n",
        "    print(f\"å…¥åŠ›: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=80)\n",
        "    print(f\"å‡ºåŠ›: {generated}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´å®Ÿé¨“\n",
        "\n",
        "ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç”ŸæˆçµæœãŒã©ã†å¤‰ã‚ã‚‹ã‹å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¯”è¼ƒå®Ÿé¨“\n",
        "prompt = \"The secret to success is\"\n",
        "\n",
        "print(\"=== æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¯”è¼ƒ ===\")\n",
        "temperatures = [0.3, 0.7, 1.0]\n",
        "\n",
        "results = []\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- æ¸©åº¦: {temp} ---\")\n",
        "    temp_results = []\n",
        "    for i in range(3):  # åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§3å›ç”Ÿæˆ\n",
        "        generated = generate_text(prompt, max_length=60, temperature=temp)\n",
        "        print(f\"{i+1}: {generated}\")\n",
        "        temp_results.append(generated)\n",
        "    results.append((temp, temp_results))\n",
        "\n",
        "# çµæœã®å¯è¦–åŒ–\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Temperature Parameter Comparison', fontsize=16)\n",
        "\n",
        "for i, (temp, temp_results) in enumerate(results):\n",
        "    axes[i].text(0.1, 0.5, f'Temp: {temp}\\n\\n' + '\\n\\n'.join(temp_results), \n",
        "                transform=axes[i].transAxes, fontsize=10, \n",
        "                verticalalignment='center', wrap=True)\n",
        "    axes[i].set_title(f'Temperature = {temp}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸºç¤\n",
        "\n",
        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹ã§ç”ŸæˆçµæœãŒå¤§ããå¤‰ã‚ã‚Šã¾ã™ã€‚æ§˜ã€…ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ä¾‹\n",
        "print(\"=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ä¾‹ ===\\n\")\n",
        "\n",
        "# ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¹ã‚¿ã‚¤ãƒ«\n",
        "prompts = {\n",
        "    \"ã‚·ãƒ³ãƒ—ãƒ«\": \"Write a story about a robot.\",\n",
        "    \"å…·ä½“çš„\": \"Write a creative science fiction story about a robot who discovers emotions.\",\n",
        "    \"ä¾‹ç¤ºä»˜ã\": \"Write a story about a robot. Here's an example: Once upon a time, there was a robot who...\",\n",
        "    \"å½¹å‰²æŒ‡å®š\": \"You are a creative writer. Write an engaging story about a robot.\",\n",
        "    \"æ§‹é€ åŒ–\": \"\"\"Write a story about a robot with the following structure:\n",
        "1. Introduction: Introduce the robot\n",
        "2. Conflict: What challenge does it face?\n",
        "3. Resolution: How does it solve the problem?\n",
        "4. Conclusion: What does it learn?\"\"\"\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for style, prompt in prompts.items():\n",
        "    print(f\"--- {style} ---\")\n",
        "    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=120, temperature=0.8)\n",
        "    print(f\"ç”Ÿæˆçµæœ: {generated}\")\n",
        "    results[style] = generated\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Few-shotå­¦ç¿’ã®å®Ÿè·µ\n",
        "\n",
        "Few-shotå­¦ç¿’ã§ã¯ã€ä¾‹ã‚’ç¤ºã™ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shotå­¦ç¿’ã®ä¾‹ï¼šæ„Ÿæƒ…åˆ†æ\n",
        "def few_shot_sentiment_analysis(text, examples):\n",
        "    \"\"\"Few-shotæ„Ÿæƒ…åˆ†æ\"\"\"\n",
        "    prompt = \"Classify the sentiment of the following texts as positive, negative, or neutral:\\n\\n\"\n",
        "    \n",
        "    # ä¾‹ã‚’è¿½åŠ \n",
        "    for example_text, example_label in examples:\n",
        "        prompt += f\"Text: {example_text}\\nSentiment: {example_label}\\n\\n\"\n",
        "    \n",
        "    # å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ \n",
        "    prompt += f\"Text: {text}\\nSentiment:\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# æ„Ÿæƒ…åˆ†æã®ä¾‹\n",
        "sentiment_examples = [\n",
        "    (\"I love this product! It's amazing!\", \"positive\"),\n",
        "    (\"This is terrible. I hate it.\", \"negative\"),\n",
        "    (\"The weather is okay today.\", \"neutral\"),\n",
        "    (\"Fantastic! Best experience ever!\", \"positive\"),\n",
        "    (\"I'm so disappointed with this service.\", \"negative\")\n",
        "]\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆ\n",
        "test_texts = [\n",
        "    \"This movie was absolutely incredible!\",\n",
        "    \"I'm not sure how I feel about this.\",\n",
        "    \"Worst purchase I've ever made.\",\n",
        "    \"It's fine, I guess.\",\n",
        "    \"Outstanding quality and great value!\"\n",
        "]\n",
        "\n",
        "print(\"=== Few-shotæ„Ÿæƒ…åˆ†æ ===\")\n",
        "for text in test_texts:\n",
        "    print(f\"\\nãƒ†ã‚­ã‚¹ãƒˆ: {text}\")\n",
        "    \n",
        "    # Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ\n",
        "    few_shot_prompt = few_shot_sentiment_analysis(text, sentiment_examples)\n",
        "    \n",
        "    # ç”Ÿæˆå®Ÿè¡Œ\n",
        "    result = generate_text(few_shot_prompt, max_length=50, temperature=0.3)\n",
        "    print(f\"Few-shotçµæœ: {result}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "\n",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªç”±ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã§ãã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ©Ÿèƒ½ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "def interactive_generation():\n",
        "    \"\"\"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\"\"\"\n",
        "    print(\"ğŸ¤– ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªLLMãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\")\n",
        "    print(\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¨å…¥åŠ›ï¼‰\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¥åŠ›\n",
        "            prompt = input(\"\\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \")\n",
        "            \n",
        "            if prompt.lower() == 'quit':\n",
        "                print(\"ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼\")\n",
        "                break\n",
        "            \n",
        "            if not prompt.strip():\n",
        "                print(\"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
        "                continue\n",
        "            \n",
        "            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
        "            print(\"\\nâš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š:\")\n",
        "            try:\n",
        "                max_length = int(input(\"æœ€å¤§é•· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100): \") or \"100\")\n",
        "                temperature = float(input(\"æ¸©åº¦ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7): \") or \"0.7\")\n",
        "            except ValueError:\n",
        "                print(\"âš ï¸ ç„¡åŠ¹ãªå€¤ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
        "                max_length = 100\n",
        "                temperature = 0.7\n",
        "            \n",
        "            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "            print(f\"\\nğŸ”„ ç”Ÿæˆä¸­... (é•·ã•: {max_length}, æ¸©åº¦: {temperature})\")\n",
        "            generated = generate_text(prompt, max_length=max_length, temperature=temperature)\n",
        "            \n",
        "            print(f\"\\nâœ¨ ç”Ÿæˆçµæœ:\")\n",
        "            print(f\"{generated}\")\n",
        "            print(\"=\" * 50)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "            continue\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç”Ÿæˆã®é–‹å§‹\n",
        "print(\"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "print(\"æ³¨æ„: Colabã§ã¯å…¥åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "print(\"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\\n\")\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œï¼ˆColabç”¨ï¼‰\n",
        "sample_prompts = [\n",
        "    \"Write a haiku about artificial intelligence:\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Create a recipe for chocolate cake:\"\n",
        "]\n",
        "\n",
        "print(\"=== ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ ===\")\n",
        "for prompt in sample_prompts:\n",
        "    print(f\"\\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=80, temperature=0.7)\n",
        "    print(f\"ç”Ÿæˆçµæœ: {generated}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨å¯è¦–åŒ–\n",
        "\n",
        "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’åˆ†æã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ\n",
        "def analyze_generation_quality(prompt, num_samples=5, temperatures=[0.3, 0.7, 1.0]):\n",
        "    \"\"\"ç”Ÿæˆå“è³ªã®åˆ†æ\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for temp in temperatures:\n",
        "        temp_results = []\n",
        "        for i in range(num_samples):\n",
        "            generated = generate_text(prompt, max_length=100, temperature=temp)\n",
        "            temp_results.append(generated)\n",
        "        results[temp] = temp_results\n",
        "    \n",
        "    return results\n",
        "\n",
        "# åˆ†æå®Ÿè¡Œ\n",
        "prompt = \"The future of technology will be\"\n",
        "print(f\"åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "quality_results = analyze_generation_quality(prompt, num_samples=3)\n",
        "\n",
        "# çµæœã®è¡¨ç¤ºã¨å¯è¦–åŒ–\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Generation Quality Analysis by Temperature', fontsize=16)\n",
        "\n",
        "for i, (temp, texts) in enumerate(quality_results.items()):\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\n",
        "    print(f\"\\n--- æ¸©åº¦ {temp} ---\")\n",
        "    for j, text in enumerate(texts, 1):\n",
        "        print(f\"{j}: {text}\")\n",
        "    \n",
        "    # å¯è¦–åŒ–\n",
        "    axes[i].text(0.05, 0.95, f'Temperature: {temp}', transform=axes[i].transAxes, \n",
        "                fontsize=14, fontweight='bold', verticalalignment='top')\n",
        "    \n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºï¼ˆé•·ã„å ´åˆã¯çœç•¥ï¼‰\n",
        "    display_text = '\\n\\n'.join([text[:100] + '...' if len(text) > 100 else text for text in texts])\n",
        "    axes[i].text(0.05, 0.85, display_text, transform=axes[i].transAxes, \n",
        "                fontsize=10, verticalalignment='top', wrap=True)\n",
        "    axes[i].set_xlim(0, 1)\n",
        "    axes[i].set_ylim(0, 1)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# çµ±è¨ˆæƒ…å ±\n",
        "print(\"\\n=== çµ±è¨ˆæƒ…å ± ===\")\n",
        "for temp, texts in quality_results.items():\n",
        "    avg_length = np.mean([len(text.split()) for text in texts])\n",
        "    print(f\"æ¸©åº¦ {temp}: å¹³å‡å˜èªæ•° {avg_length:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã®ä½“é¨“\n",
        "\n",
        "GPT-2ã®ä»–ã«ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚‚è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚Colabã®åˆ¶é™å†…ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ\n",
        "def load_and_compare_models():\n",
        "    \"\"\"è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ\"\"\"\n",
        "    models_to_try = [\n",
        "        (\"gpt2\", \"GPT-2 (117M parameters)\"),\n",
        "        (\"gpt2-medium\", \"GPT-2 Medium (345M parameters)\"),\n",
        "        # (\"gpt2-large\", \"GPT-2 Large (762M parameters)\"),  # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
        "        # (\"gpt2-xl\", \"GPT-2 XL (1.5B parameters)\")  # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name, description in models_to_try:\n",
        "        try:\n",
        "            print(f\"\\nğŸ”„ {description} ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "            \n",
        "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            \n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            \n",
        "            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            \n",
        "            # ãƒ†ã‚¹ãƒˆç”Ÿæˆ\n",
        "            test_prompt = \"The future of artificial intelligence is\"\n",
        "            generated = generate_text_with_model(model, tokenizer, test_prompt, device)\n",
        "            \n",
        "            results[model_name] = {\n",
        "                'description': description,\n",
        "                'parameters': model.num_parameters(),\n",
        "                'generated': generated\n",
        "            }\n",
        "            \n",
        "            print(f\"âœ… {description} å®Œäº†\")\n",
        "            print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}\")\n",
        "            print(f\"ç”Ÿæˆçµæœ: {generated[:100]}...\")\n",
        "            \n",
        "            # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "            del model, tokenizer\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {description} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "def generate_text_with_model(model, tokenizer, prompt, device, max_length=80, temperature=0.7):\n",
        "    \"\"\"æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\"\"\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®å®Ÿè¡Œ\n",
        "print(\"=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ ===\")\n",
        "print(\"æ³¨æ„: å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯ãƒ¡ãƒ¢ãƒªã‚’å¤šãä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
        "print(\"Colabã®åˆ¶é™ã«ã‚ˆã‚Šã€ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯èª­ã¿è¾¼ã‚ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\\n\")\n",
        "\n",
        "model_results = load_and_compare_models()\n",
        "\n",
        "# çµæœã®å¯è¦–åŒ–\n",
        "if model_results:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    model_names = list(model_results.keys())\n",
        "    param_counts = [model_results[name]['parameters'] for name in model_names]\n",
        "    descriptions = [model_results[name]['description'] for name in model_names]\n",
        "    \n",
        "    bars = ax.bar(range(len(model_names)), param_counts, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Number of Parameters')\n",
        "    ax.set_title('Model Size Comparison')\n",
        "    ax.set_xticks(range(len(model_names)))\n",
        "    ax.set_xticklabels([name.split('-')[0] for name in model_names], rotation=45)\n",
        "    \n",
        "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
        "    for i, (bar, count) in enumerate(zip(bars, param_counts)):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.01,\n",
        "                f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ç”Ÿæˆçµæœã®æ¯”è¼ƒ\n",
        "    print(\"\\n=== ç”Ÿæˆçµæœã®æ¯”è¼ƒ ===\")\n",
        "    test_prompt = \"The future of artificial intelligence is\"\n",
        "    for model_name, result in model_results.items():\n",
        "        print(f\"\\n{result['description']}:\")\n",
        "        print(f\"ç”Ÿæˆçµæœ: {result['generated']}\")\n",
        "else:\n",
        "    print(\"âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. å­¦ç¿’ã®ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã‚“ã ã“ã¨ã‚’ã¾ã¨ã‚ã¦ã€æ¬¡ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’ã®ã¾ã¨ã‚\n",
        "print(\"ğŸ“ LLMåŸºç¤å­¦ç¿’ã®ã¾ã¨ã‚\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "learned_concepts = [\n",
        "    \"âœ… LLMã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã¨ä½¿ã„æ–¹\",\n",
        "    \"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŸºç¤\",\n",
        "    \"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆæ¸©åº¦ã€top_pç­‰ï¼‰ã®å½±éŸ¿\",\n",
        "    \"âœ… Few-shotå­¦ç¿’ã®å®Ÿè·µ\",\n",
        "    \"âœ… ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\",\n",
        "    \"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨å¯è¦–åŒ–\",\n",
        "    \"âœ… è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“\"\n",
        "]\n",
        "\n",
        "print(\"ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ç¿’ã—ãŸå†…å®¹:\")\n",
        "for concept in learned_concepts:\n",
        "    print(f\"  {concept}\")\n",
        "\n",
        "print(\"\\nğŸ” é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:\")\n",
        "key_points = [\n",
        "    \"â€¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹ãŒç”Ÿæˆå“è³ªã«å¤§ããå½±éŸ¿ã™ã‚‹\",\n",
        "    \"â€¢ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å‰µé€ æ€§ã‚’èª¿æ•´ã§ãã‚‹\",\n",
        "    \"â€¢ Few-shotå­¦ç¿’ã§ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã§ãã‚‹\",\n",
        "    \"â€¢ ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã»ã©é«˜å“è³ªãªç”ŸæˆãŒå¯èƒ½\",\n",
        "    \"â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã¯ç”¨é€”ã«å¿œã˜ã¦è¡Œã†\"\n",
        "]\n",
        "\n",
        "for point in key_points:\n",
        "    print(f\"  {point}\")\n",
        "\n",
        "print(\"\\nğŸ“š æ¬¡ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
        "next_steps = [\n",
        "    \"1. ã‚ˆã‚Šé«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\",\n",
        "    \"2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè·µ\",\n",
        "    \"3. æ„Ÿæƒ…åˆ†æã‚„åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…\",\n",
        "    \"4. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3.5/4ã€Claudeç­‰ï¼‰ã®ä½¿ç”¨\",\n",
        "    \"5. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã®å­¦ç¿’\",\n",
        "    \"6. LLMã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¸ã®å‚åŠ \"\n",
        "]\n",
        "\n",
        "for step in next_steps:\n",
        "    print(f\"  {step}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹:\")\n",
        "advice = [\n",
        "    \"â€¢ æ¯æ—¥å°‘ã—ãšã¤ã§ã‚‚LLMã‚’ä½¿ã„ç¶šã‘ã‚‹\",\n",
        "    \"â€¢ æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’è©¦ã—ã¦ã¿ã‚‹\",\n",
        "    \"â€¢ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼ˆDiscordã€Twitterç­‰ï¼‰ã«å‚åŠ ã™ã‚‹\",\n",
        "    \"â€¢ å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§LLMã‚’æ´»ç”¨ã™ã‚‹\",\n",
        "    \"â€¢ æœ€æ–°ã®ç ”ç©¶è«–æ–‡ã‚’èª­ã‚“ã§çŸ¥è­˜ã‚’æ›´æ–°ã™ã‚‹\"\n",
        "]\n",
        "\n",
        "for tip in advice:\n",
        "    print(f\"  {tip}\")\n",
        "\n",
        "print(\"\\nğŸŒŸ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\")\n",
        "print(\"LLMã®åŸºç¤å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "print(\"ã“ã‚Œã‹ã‚‰ã¯å®Ÿè·µã‚’é€šã˜ã¦ã•ã‚‰ã«ã‚¹ã‚­ãƒ«ã‚’å‘ä¸Šã•ã›ã¦ã„ãã¾ã—ã‚‡ã†ï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹ã¨å‚è€ƒè³‡æ–™\n",
        "\n",
        "### æœ‰ç”¨ãªãƒªã‚½ãƒ¼ã‚¹\n",
        "- **Hugging Face Hub**: https://huggingface.co/models\n",
        "- **Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: https://huggingface.co/docs/transformers/\n",
        "- **OpenAI API**: https://platform.openai.com/\n",
        "- **Anthropic Claude**: https://www.anthropic.com/\n",
        "\n",
        "### å­¦ç¿’ã«å½¹ç«‹ã¤æ›¸ç±ãƒ»è«–æ–‡\n",
        "- \"Attention Is All You Need\" (Transformerè«–æ–‡)\n",
        "- \"Language Models are Few-Shot Learners\" (GPT-3è«–æ–‡)\n",
        "- \"Training language models to follow instructions\" (InstructGPTè«–æ–‡)\n",
        "\n",
        "### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n",
        "- **Hugging Face Discord**: æœ€æ–°ã®æŠ€è¡“æƒ…å ±äº¤æ›\n",
        "- **r/MachineLearning**: Redditã®æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n",
        "- **Twitter**: #LLM #NLP ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§æœ€æ–°æƒ…å ±ã‚’ãƒ•ã‚©ãƒ­ãƒ¼\n",
        "\n",
        "### å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ã‚¤ãƒ‡ã‚¢\n",
        "1. **ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™º**: ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ç‰¹åŒ–ã—ãŸå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ \n",
        "2. **ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãƒ„ãƒ¼ãƒ«**: é•·ã„æ–‡ç« ã‚’è‡ªå‹•ã§è¦ç´„\n",
        "3. **ç¿»è¨³ã‚¢ãƒ—ãƒª**: å¤šè¨€èªå¯¾å¿œã®ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ \n",
        "4. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ**: ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–‡æ›¸ã®è‡ªå‹•ç”Ÿæˆ\n",
        "5. **ã‚³ãƒ¼ãƒ‰ç”Ÿæˆæ”¯æ´**: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è£œåŠ©ãƒ„ãƒ¼ãƒ«\n",
        "\n",
        "---\n",
        "\n",
        "**ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Œäº†ã—ãŸã‚‰ã€ãœã²GitHubã«ä¿å­˜ã—ã¦ã€ä»–ã®äººã¨å…±æœ‰ã—ã¦ãã ã•ã„ï¼**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
