{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM基礎実習: Google Colab版\n",
        "\n",
        "このノートブックは、Google ColabでLLMの基本的な使い方を学ぶためのものです。\n",
        "\n",
        "## 学習目標\n",
        "- Google ColabでLLM環境をセットアップする\n",
        "- 簡単なLLMモデルを読み込んで使用する\n",
        "- プロンプトエンジニアリングの基礎を理解する\n",
        "- テキスト生成の基本的な仕組みを体験する\n",
        "\n",
        "## 注意事項\n",
        "- このノートブックはGoogle Colabで実行することを前提としています\n",
        "- GPUを使用する場合は、ランタイム > ランタイムのタイプを変更 > GPU を選択してください\n",
        "- 無料版では制限がありますが、基本的な学習には十分です\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 環境セットアップ\n",
        "\n",
        "まず、必要なライブラリをインストールして環境をセットアップします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets accelerate evaluate scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "\n",
        "# インストール確認\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 日本語フォントの設定（可視化用）\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"✅ ライブラリのインポートが完了しました\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 簡単なLLMモデルの読み込み\n",
        "\n",
        "まずは、比較的軽量なGPT-2モデルを使ってみましょう。Colabでは、モデルのダウンロードに時間がかかる場合があります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# モデルとトークナイザーの読み込み\n",
        "model_name = \"gpt2\"  # 軽量なGPT-2モデル\n",
        "\n",
        "print(f\"🔄 {model_name} を読み込み中...\")\n",
        "print(\"初回実行時は、モデルのダウンロードに時間がかかります。\")\n",
        "\n",
        "# トークナイザーの読み込み\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# モデルの読み込み\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# パディングトークンを設定\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✅ モデルの読み込みが完了しました！\")\n",
        "print(f\"📊 モデルパラメータ数: {model.num_parameters():,}\")\n",
        "print(f\"🔤 語彙数: {len(tokenizer)}\")\n",
        "\n",
        "# デバイスの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"🖥️ 使用デバイス: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 基本的なテキスト生成\n",
        "\n",
        "シンプルなプロンプトからテキストを生成してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    テキストを生成する関数\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): 入力プロンプト\n",
        "        max_length (int): 最大生成長\n",
        "        temperature (float): 生成のランダム性（0.0-1.0）\n",
        "        top_p (float): 核サンプリングのパラメータ\n",
        "    \"\"\"\n",
        "    # テキストをトークン化\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # テキスト生成\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    # 生成されたテキストをデコード\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# テスト用のプロンプト\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Once upon a time, there was a\",\n",
        "    \"In the world of machine learning,\",\n",
        "    \"The secret to success is\",\n",
        "    \"Climate change is one of the most\"\n",
        "]\n",
        "\n",
        "print(\"=== 基本的なテキスト生成 ===\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- プロンプト {i} ---\")\n",
        "    print(f\"入力: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=80)\n",
        "    print(f\"出力: {generated}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. パラメータの調整実験\n",
        "\n",
        "異なるパラメータで生成結果がどう変わるか実験してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 温度パラメータの比較実験\n",
        "prompt = \"The secret to success is\"\n",
        "\n",
        "print(\"=== 温度パラメータの比較 ===\")\n",
        "temperatures = [0.3, 0.7, 1.0]\n",
        "\n",
        "results = []\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- 温度: {temp} ---\")\n",
        "    temp_results = []\n",
        "    for i in range(3):  # 同じプロンプトで3回生成\n",
        "        generated = generate_text(prompt, max_length=60, temperature=temp)\n",
        "        print(f\"{i+1}: {generated}\")\n",
        "        temp_results.append(generated)\n",
        "    results.append((temp, temp_results))\n",
        "\n",
        "# 結果の可視化\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Temperature Parameter Comparison', fontsize=16)\n",
        "\n",
        "for i, (temp, temp_results) in enumerate(results):\n",
        "    axes[i].text(0.1, 0.5, f'Temp: {temp}\\n\\n' + '\\n\\n'.join(temp_results), \n",
        "                transform=axes[i].transAxes, fontsize=10, \n",
        "                verticalalignment='center', wrap=True)\n",
        "    axes[i].set_title(f'Temperature = {temp}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. プロンプトエンジニアリングの基礎\n",
        "\n",
        "プロンプトの書き方で生成結果が大きく変わります。様々なテクニックを試してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# プロンプトエンジニアリングの例\n",
        "print(\"=== プロンプトエンジニアリングの例 ===\\n\")\n",
        "\n",
        "# 異なるプロンプトスタイル\n",
        "prompts = {\n",
        "    \"シンプル\": \"Write a story about a robot.\",\n",
        "    \"具体的\": \"Write a creative science fiction story about a robot who discovers emotions.\",\n",
        "    \"例示付き\": \"Write a story about a robot. Here's an example: Once upon a time, there was a robot who...\",\n",
        "    \"役割指定\": \"You are a creative writer. Write an engaging story about a robot.\",\n",
        "    \"構造化\": \"\"\"Write a story about a robot with the following structure:\n",
        "1. Introduction: Introduce the robot\n",
        "2. Conflict: What challenge does it face?\n",
        "3. Resolution: How does it solve the problem?\n",
        "4. Conclusion: What does it learn?\"\"\"\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for style, prompt in prompts.items():\n",
        "    print(f\"--- {style} ---\")\n",
        "    print(f\"プロンプト: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=120, temperature=0.8)\n",
        "    print(f\"生成結果: {generated}\")\n",
        "    results[style] = generated\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Few-shot学習の実践\n",
        "\n",
        "Few-shot学習では、例を示すことでモデルの性能を向上させることができます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot学習の例：感情分析\n",
        "def few_shot_sentiment_analysis(text, examples):\n",
        "    \"\"\"Few-shot感情分析\"\"\"\n",
        "    prompt = \"Classify the sentiment of the following texts as positive, negative, or neutral:\\n\\n\"\n",
        "    \n",
        "    # 例を追加\n",
        "    for example_text, example_label in examples:\n",
        "        prompt += f\"Text: {example_text}\\nSentiment: {example_label}\\n\\n\"\n",
        "    \n",
        "    # 対象テキストを追加\n",
        "    prompt += f\"Text: {text}\\nSentiment:\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# 感情分析の例\n",
        "sentiment_examples = [\n",
        "    (\"I love this product! It's amazing!\", \"positive\"),\n",
        "    (\"This is terrible. I hate it.\", \"negative\"),\n",
        "    (\"The weather is okay today.\", \"neutral\"),\n",
        "    (\"Fantastic! Best experience ever!\", \"positive\"),\n",
        "    (\"I'm so disappointed with this service.\", \"negative\")\n",
        "]\n",
        "\n",
        "# テスト用のテキスト\n",
        "test_texts = [\n",
        "    \"This movie was absolutely incredible!\",\n",
        "    \"I'm not sure how I feel about this.\",\n",
        "    \"Worst purchase I've ever made.\",\n",
        "    \"It's fine, I guess.\",\n",
        "    \"Outstanding quality and great value!\"\n",
        "]\n",
        "\n",
        "print(\"=== Few-shot感情分析 ===\")\n",
        "for text in test_texts:\n",
        "    print(f\"\\nテキスト: {text}\")\n",
        "    \n",
        "    # Few-shotプロンプトの生成\n",
        "    few_shot_prompt = few_shot_sentiment_analysis(text, sentiment_examples)\n",
        "    \n",
        "    # 生成実行\n",
        "    result = generate_text(few_shot_prompt, max_length=50, temperature=0.3)\n",
        "    print(f\"Few-shot結果: {result}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. インタラクティブなテキスト生成\n",
        "\n",
        "ユーザーが自由にプロンプトを入力できるインタラクティブな機能を作成しましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# インタラクティブなテキスト生成\n",
        "def interactive_generation():\n",
        "    \"\"\"インタラクティブなテキスト生成\"\"\"\n",
        "    print(\"🤖 インタラクティブなLLMテキスト生成\")\n",
        "    print(\"プロンプトを入力してください（終了するには 'quit' と入力）\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # プロンプトの入力\n",
        "            prompt = input(\"\\n💬 プロンプト: \")\n",
        "            \n",
        "            if prompt.lower() == 'quit':\n",
        "                print(\"👋 終了します。お疲れ様でした！\")\n",
        "                break\n",
        "            \n",
        "            if not prompt.strip():\n",
        "                print(\"⚠️ プロンプトを入力してください。\")\n",
        "                continue\n",
        "            \n",
        "            # パラメータの設定\n",
        "            print(\"\\n⚙️ パラメータ設定:\")\n",
        "            try:\n",
        "                max_length = int(input(\"最大長 (デフォルト: 100): \") or \"100\")\n",
        "                temperature = float(input(\"温度 (デフォルト: 0.7): \") or \"0.7\")\n",
        "            except ValueError:\n",
        "                print(\"⚠️ 無効な値です。デフォルト値を使用します。\")\n",
        "                max_length = 100\n",
        "                temperature = 0.7\n",
        "            \n",
        "            # テキスト生成\n",
        "            print(f\"\\n🔄 生成中... (長さ: {max_length}, 温度: {temperature})\")\n",
        "            generated = generate_text(prompt, max_length=max_length, temperature=temperature)\n",
        "            \n",
        "            print(f\"\\n✨ 生成結果:\")\n",
        "            print(f\"{generated}\")\n",
        "            print(\"=\" * 50)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n👋 終了します。お疲れ様でした！\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ エラーが発生しました: {e}\")\n",
        "            continue\n",
        "\n",
        "# インタラクティブ生成の開始\n",
        "print(\"インタラクティブなテキスト生成を開始します...\")\n",
        "print(\"注意: Colabでは入力が制限される場合があります。\")\n",
        "print(\"ローカル環境で実行することをお勧めします。\\n\")\n",
        "\n",
        "# サンプル実行（Colab用）\n",
        "sample_prompts = [\n",
        "    \"Write a haiku about artificial intelligence:\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Create a recipe for chocolate cake:\"\n",
        "]\n",
        "\n",
        "print(\"=== サンプル実行 ===\")\n",
        "for prompt in sample_prompts:\n",
        "    print(f\"\\nプロンプト: {prompt}\")\n",
        "    generated = generate_text(prompt, max_length=80, temperature=0.7)\n",
        "    print(f\"生成結果: {generated}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. パフォーマンス分析と可視化\n",
        "\n",
        "生成されたテキストの品質やパラメータの影響を分析してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# パフォーマンス分析\n",
        "def analyze_generation_quality(prompt, num_samples=5, temperatures=[0.3, 0.7, 1.0]):\n",
        "    \"\"\"生成品質の分析\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for temp in temperatures:\n",
        "        temp_results = []\n",
        "        for i in range(num_samples):\n",
        "            generated = generate_text(prompt, max_length=100, temperature=temp)\n",
        "            temp_results.append(generated)\n",
        "        results[temp] = temp_results\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 分析実行\n",
        "prompt = \"The future of technology will be\"\n",
        "print(f\"分析プロンプト: {prompt}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "quality_results = analyze_generation_quality(prompt, num_samples=3)\n",
        "\n",
        "# 結果の表示と可視化\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Generation Quality Analysis by Temperature', fontsize=16)\n",
        "\n",
        "for i, (temp, texts) in enumerate(quality_results.items()):\n",
        "    # テキストの表示\n",
        "    print(f\"\\n--- 温度 {temp} ---\")\n",
        "    for j, text in enumerate(texts, 1):\n",
        "        print(f\"{j}: {text}\")\n",
        "    \n",
        "    # 可視化\n",
        "    axes[i].text(0.05, 0.95, f'Temperature: {temp}', transform=axes[i].transAxes, \n",
        "                fontsize=14, fontweight='bold', verticalalignment='top')\n",
        "    \n",
        "    # テキストを表示（長い場合は省略）\n",
        "    display_text = '\\n\\n'.join([text[:100] + '...' if len(text) > 100 else text for text in texts])\n",
        "    axes[i].text(0.05, 0.85, display_text, transform=axes[i].transAxes, \n",
        "                fontsize=10, verticalalignment='top', wrap=True)\n",
        "    axes[i].set_xlim(0, 1)\n",
        "    axes[i].set_ylim(0, 1)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 統計情報\n",
        "print(\"\\n=== 統計情報 ===\")\n",
        "for temp, texts in quality_results.items():\n",
        "    avg_length = np.mean([len(text.split()) for text in texts])\n",
        "    print(f\"温度 {temp}: 平均単語数 {avg_length:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. より大きなモデルの体験\n",
        "\n",
        "GPT-2の他に、より大きなモデルも試してみましょう。Colabの制限内で利用可能なモデルを紹介します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# より大きなモデルの比較\n",
        "def load_and_compare_models():\n",
        "    \"\"\"複数のモデルを比較\"\"\"\n",
        "    models_to_try = [\n",
        "        (\"gpt2\", \"GPT-2 (117M parameters)\"),\n",
        "        (\"gpt2-medium\", \"GPT-2 Medium (345M parameters)\"),\n",
        "        # (\"gpt2-large\", \"GPT-2 Large (762M parameters)\"),  # メモリ制限のためコメントアウト\n",
        "        # (\"gpt2-xl\", \"GPT-2 XL (1.5B parameters)\")  # メモリ制限のためコメントアウト\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name, description in models_to_try:\n",
        "        try:\n",
        "            print(f\"\\n🔄 {description} を読み込み中...\")\n",
        "            \n",
        "            # トークナイザーとモデルの読み込み\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            \n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            \n",
        "            # デバイスに移動\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            \n",
        "            # テスト生成\n",
        "            test_prompt = \"The future of artificial intelligence is\"\n",
        "            generated = generate_text_with_model(model, tokenizer, test_prompt, device)\n",
        "            \n",
        "            results[model_name] = {\n",
        "                'description': description,\n",
        "                'parameters': model.num_parameters(),\n",
        "                'generated': generated\n",
        "            }\n",
        "            \n",
        "            print(f\"✅ {description} 完了\")\n",
        "            print(f\"パラメータ数: {model.num_parameters():,}\")\n",
        "            print(f\"生成結果: {generated[:100]}...\")\n",
        "            \n",
        "            # メモリ解放\n",
        "            del model, tokenizer\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ {description} の読み込みに失敗: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "def generate_text_with_model(model, tokenizer, prompt, device, max_length=80, temperature=0.7):\n",
        "    \"\"\"指定されたモデルでテキスト生成\"\"\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# モデル比較の実行\n",
        "print(\"=== モデル比較実験 ===\")\n",
        "print(\"注意: 大きなモデルはメモリを多く使用します。\")\n",
        "print(\"Colabの制限により、一部のモデルは読み込めない場合があります。\\n\")\n",
        "\n",
        "model_results = load_and_compare_models()\n",
        "\n",
        "# 結果の可視化\n",
        "if model_results:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    model_names = list(model_results.keys())\n",
        "    param_counts = [model_results[name]['parameters'] for name in model_names]\n",
        "    descriptions = [model_results[name]['description'] for name in model_names]\n",
        "    \n",
        "    bars = ax.bar(range(len(model_names)), param_counts, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Number of Parameters')\n",
        "    ax.set_title('Model Size Comparison')\n",
        "    ax.set_xticks(range(len(model_names)))\n",
        "    ax.set_xticklabels([name.split('-')[0] for name in model_names], rotation=45)\n",
        "    \n",
        "    # パラメータ数をバーの上に表示\n",
        "    for i, (bar, count) in enumerate(zip(bars, param_counts)):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.01,\n",
        "                f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 生成結果の比較\n",
        "    print(\"\\n=== 生成結果の比較 ===\")\n",
        "    test_prompt = \"The future of artificial intelligence is\"\n",
        "    for model_name, result in model_results.items():\n",
        "        print(f\"\\n{result['description']}:\")\n",
        "        print(f\"生成結果: {result['generated']}\")\n",
        "else:\n",
        "    print(\"❌ 利用可能なモデルがありませんでした。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 学習のまとめと次のステップ\n",
        "\n",
        "このノートブックで学んだことをまとめて、次の学習ステップを確認しましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 学習のまとめ\n",
        "print(\"🎓 LLM基礎学習のまとめ\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "learned_concepts = [\n",
        "    \"✅ LLMの基本的な仕組みと使い方\",\n",
        "    \"✅ プロンプトエンジニアリングの基礎\",\n",
        "    \"✅ パラメータ調整（温度、top_p等）の影響\",\n",
        "    \"✅ Few-shot学習の実践\",\n",
        "    \"✅ インタラクティブなテキスト生成\",\n",
        "    \"✅ パフォーマンス分析と可視化\",\n",
        "    \"✅ 複数モデルの比較実験\"\n",
        "]\n",
        "\n",
        "print(\"このノートブックで学習した内容:\")\n",
        "for concept in learned_concepts:\n",
        "    print(f\"  {concept}\")\n",
        "\n",
        "print(\"\\n🔍 重要なポイント:\")\n",
        "key_points = [\n",
        "    \"• プロンプトの書き方が生成品質に大きく影響する\",\n",
        "    \"• 温度パラメータで創造性を調整できる\",\n",
        "    \"• Few-shot学習で特定のタスクに特化できる\",\n",
        "    \"• より大きなモデルほど高品質な生成が可能\",\n",
        "    \"• パラメータの調整は用途に応じて行う\"\n",
        "]\n",
        "\n",
        "for point in key_points:\n",
        "    print(f\"  {point}\")\n",
        "\n",
        "print(\"\\n📚 次の学習ステップ:\")\n",
        "next_steps = [\n",
        "    \"1. より高度なプロンプトエンジニアリング\",\n",
        "    \"2. ファインチューニングの実践\",\n",
        "    \"3. 感情分析や分類タスクの実装\",\n",
        "    \"4. より大きなモデル（GPT-3.5/4、Claude等）の使用\",\n",
        "    \"5. マルチモーダル（画像+テキスト）の学習\",\n",
        "    \"6. LLMコンペティションへの参加\"\n",
        "]\n",
        "\n",
        "for step in next_steps:\n",
        "    print(f\"  {step}\")\n",
        "\n",
        "print(\"\\n💡 実践的なアドバイス:\")\n",
        "advice = [\n",
        "    \"• 毎日少しずつでもLLMを使い続ける\",\n",
        "    \"• 新しいプロンプトテクニックを試してみる\",\n",
        "    \"• コミュニティ（Discord、Twitter等）に参加する\",\n",
        "    \"• 実際のプロジェクトでLLMを活用する\",\n",
        "    \"• 最新の研究論文を読んで知識を更新する\"\n",
        "]\n",
        "\n",
        "for tip in advice:\n",
        "    print(f\"  {tip}\")\n",
        "\n",
        "print(\"\\n🌟 おめでとうございます！\")\n",
        "print(\"LLMの基礎学習が完了しました。\")\n",
        "print(\"これからは実践を通じてさらにスキルを向上させていきましょう！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 追加リソースと参考資料\n",
        "\n",
        "### 有用なリソース\n",
        "- **Hugging Face Hub**: https://huggingface.co/models\n",
        "- **Transformers ライブラリ**: https://huggingface.co/docs/transformers/\n",
        "- **OpenAI API**: https://platform.openai.com/\n",
        "- **Anthropic Claude**: https://www.anthropic.com/\n",
        "\n",
        "### 学習に役立つ書籍・論文\n",
        "- \"Attention Is All You Need\" (Transformer論文)\n",
        "- \"Language Models are Few-Shot Learners\" (GPT-3論文)\n",
        "- \"Training language models to follow instructions\" (InstructGPT論文)\n",
        "\n",
        "### コミュニティ\n",
        "- **Hugging Face Discord**: 最新の技術情報交換\n",
        "- **r/MachineLearning**: Redditの機械学習コミュニティ\n",
        "- **Twitter**: #LLM #NLP ハッシュタグで最新情報をフォロー\n",
        "\n",
        "### 実践的なプロジェクトアイデア\n",
        "1. **チャットボット開発**: 特定のドメインに特化した対話システム\n",
        "2. **テキスト要約ツール**: 長い文章を自動で要約\n",
        "3. **翻訳アプリ**: 多言語対応の翻訳システム\n",
        "4. **コンテンツ生成**: ブログ記事やマーケティング文書の自動生成\n",
        "5. **コード生成支援**: プログラミングの補助ツール\n",
        "\n",
        "---\n",
        "\n",
        "**このノートブックを完了したら、ぜひGitHubに保存して、他の人と共有してください！**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
