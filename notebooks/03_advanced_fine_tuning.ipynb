{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# é«˜åº¦ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿç¿’\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
        "\n",
        "## å­¦ç¿’ç›®æ¨™\n",
        "- ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "- ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒ\n",
        "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
        "- ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨åˆ†æ\n",
        "- å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®å¿œç”¨\n",
        "\n",
        "## å‰ææ¡ä»¶\n",
        "- LLMåŸºç¤å®Ÿç¿’ã®å®Œäº†\n",
        "- PyTorchã¨Transformersã®åŸºæœ¬ç†è§£\n",
        "- æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤çŸ¥è­˜\n",
        "\n",
        "## æ³¨æ„äº‹é …\n",
        "- ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯è¨ˆç®—é›†ç´„çš„ã§ã™\n",
        "- GPUã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™\n",
        "- å®Ÿè¡Œæ™‚é–“ã¯æ•°æ™‚é–“ã«åŠã¶å ´åˆãŒã‚ã‚Šã¾ã™\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†\n",
        "        if isinstance(text, str):\n",
        "            # å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        else:\n",
        "            # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ\n",
        "            formatted_text = self.format_text(text)\n",
        "            encoding = self.tokenizer(\n",
        "                formatted_text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "    def format_text(self, text_dict):\n",
        "        \"\"\"æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\"\"\"\n",
        "        if 'instruction' in text_dict and 'response' in text_dict:\n",
        "            return f\"Instruction: {text_dict['instruction']}\\\\nResponse: {text_dict['response']}\"\n",
        "        elif 'context' in text_dict and 'question' in text_dict:\n",
        "            return f\"Context: {text_dict['context']}\\\\nQuestion: {text_dict['question']}\"\n",
        "        else:\n",
        "            return str(text_dict)\n",
        "\n",
        "def create_sample_datasets():\n",
        "    \"\"\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\"\"\"\n",
        "    \n",
        "    # 1. æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "    sentiment_data = {\n",
        "        'texts': [\n",
        "            \"I love this product! It's amazing!\",\n",
        "            \"This is terrible. I hate it.\",\n",
        "            \"The weather is okay today.\",\n",
        "            \"Fantastic! Best experience ever!\",\n",
        "            \"I'm so disappointed with this service.\",\n",
        "            \"It's fine, I guess.\",\n",
        "            \"Outstanding quality and great value!\",\n",
        "            \"Worst purchase I've ever made.\",\n",
        "            \"Good product, would recommend.\",\n",
        "            \"Excellent service and fast delivery!\"\n",
        "        ],\n",
        "        'labels': [1, 0, 1, 1, 0, 1, 1, 0, 1, 1]  # 0: negative, 1: positive\n",
        "    }\n",
        "    \n",
        "    # 2. è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "    qa_data = {\n",
        "        'texts': [\n",
        "            {\n",
        "                'context': 'Machine learning is a subset of artificial intelligence.',\n",
        "                'question': 'What is machine learning?'\n",
        "            },\n",
        "            {\n",
        "                'context': 'Python is a popular programming language for data science.',\n",
        "                'question': 'Which language is good for data science?'\n",
        "            },\n",
        "            {\n",
        "                'context': 'Deep learning uses neural networks with multiple layers.',\n",
        "                'question': 'How does deep learning work?'\n",
        "            }\n",
        "        ],\n",
        "        'labels': [0, 1, 2]  # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡\n",
        "    }\n",
        "    \n",
        "    # 3. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "    generation_data = {\n",
        "        'texts': [\n",
        "            {\n",
        "                'instruction': 'Write a haiku about artificial intelligence',\n",
        "                'response': 'Silent circuits hum\\\\nLearning patterns in the void\\\\nMind awakens new'\n",
        "            },\n",
        "            {\n",
        "                'instruction': 'Explain quantum computing simply',\n",
        "                'response': 'Quantum computing uses quantum bits that can exist in multiple states simultaneously, allowing for parallel processing.'\n",
        "            },\n",
        "            {\n",
        "                'instruction': 'Create a recipe for chocolate cake',\n",
        "                'response': 'Mix flour, sugar, cocoa powder, baking powder, and salt. Add eggs, milk, oil, and vanilla. Bake at 350Â°F for 30 minutes.'\n",
        "            }\n",
        "        ],\n",
        "        'labels': [0, 1, 2]  # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—\n",
        "    }\n",
        "    \n",
        "    return sentiment_data, qa_data, generation_data\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
        "sentiment_data, qa_data, generation_data = create_sample_datasets()\n",
        "\n",
        "print(\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä½œæˆã•ã‚Œã¾ã—ãŸ\")\n",
        "print(f\"æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿: {len(sentiment_data['texts'])}ä»¶\")\n",
        "print(f\"è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿: {len(qa_data['texts'])}ä»¶\")\n",
        "print(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¼ã‚¿: {len(generation_data['texts'])}ä»¶\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets accelerate evaluate scikit-learn pandas numpy matplotlib seaborn tqdm wandb\n",
        "!pip install peft bitsandbytes  # LoRAç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "\n",
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset as HFDataset, load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ç’°å¢ƒè¨­å®š\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
