"""
LLMåŸºç¤æ¼”ç¿’å•é¡Œ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€LLMã®åŸºç¤ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®æ¼”ç¿’å•é¡ŒãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
å„å•é¡Œã¯æ®µéšçš„ã«é›£æ˜“åº¦ãŒä¸ŠãŒã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

class LLMExerciseSolver:
    """LLMæ¼”ç¿’å•é¡Œè§£æ±ºã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name="gpt2"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def exercise_1_basic_generation(self):
        """æ¼”ç¿’1: åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        print("=== æ¼”ç¿’1: åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ ===")
        print("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        
        prompts = [
            "The future of artificial intelligence is",
            "Once upon a time, there was a",
            "In the world of machine learning,"
        ]
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\n--- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i} ---")
            print(f"å…¥åŠ›: {prompt}")
            
            # ã“ã“ã«ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„
            # ãƒ’ãƒ³ãƒˆ: generate_texté–¢æ•°ã‚’ä½¿ç”¨
            
            # è§£ç­”ä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
            # generated = self.generate_text(prompt, max_length=80)
            # print(f"å‡ºåŠ›: {generated}")
    
    def exercise_2_parameter_tuning(self):
        """æ¼”ç¿’2: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´"""
        print("\n=== æ¼”ç¿’2: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ ===")
        print("åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç•°ãªã‚‹æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        
        prompt = "The secret to success is"
        temperatures = [0.3, 0.7, 1.0]
        
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        
        for temp in temperatures:
            print(f"\næ¸©åº¦: {temp}")
            # ã“ã“ã«ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„
            # ãƒ’ãƒ³ãƒˆ: temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´
    
    def exercise_3_prompt_engineering(self):
        """æ¼”ç¿’3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("\n=== æ¼”ç¿’3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===")
        print("åŒã˜ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
        
        task = "æ„Ÿæƒ…åˆ†æ"
        text = "I absolutely love this new product!"
        
        prompt_styles = [
            f"Classify the sentiment: {text}",
            f"Determine if this is positive or negative: {text}",
            f"Analyze the emotion in this text: {text}",
            f"Rate the sentiment (positive/negative/neutral): {text}"
        ]
        
        for i, prompt in enumerate(prompt_styles, 1):
            print(f"\n--- ã‚¹ã‚¿ã‚¤ãƒ« {i} ---")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            # ã“ã“ã«ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„
    
    def exercise_4_few_shot_learning(self):
        """æ¼”ç¿’4: Few-shotå­¦ç¿’"""
        print("\n=== æ¼”ç¿’4: Few-shotå­¦ç¿’ ===")
        print("ä¾‹ç¤ºã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        
        examples = [
            ("I love this movie!", "positive"),
            ("This is terrible.", "negative"),
            ("It's okay.", "neutral")
        ]
        
        query = "This is amazing!"
        
        # ã“ã“ã«Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„
        # ãƒ’ãƒ³ãƒˆ: ä¾‹ç¤ºã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    
    def exercise_5_text_analysis(self):
        """æ¼”ç¿’5: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        print("\n=== æ¼”ç¿’5: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ ===")
        print("ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚")
        
        prompt = "Write a short story about a robot"
        generated_text = self.generate_text(prompt, max_length=150)
        
        print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {generated_text}")
        
        # ã“ã“ã«åˆ†æã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„
        # ãƒ’ãƒ³ãƒˆ: æ–‡å­—æ•°ã€å˜èªæ•°ã€æ–‡æ•°ãªã©ã‚’è¨ˆç®—
    
    def generate_text(self, prompt, max_length=100, temperature=0.7, **kwargs):
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text
    
    def run_all_exercises(self):
        """ã™ã¹ã¦ã®æ¼”ç¿’ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ LLMåŸºç¤æ¼”ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        print("=" * 50)
        
        self.exercise_1_basic_generation()
        self.exercise_2_parameter_tuning()
        self.exercise_3_prompt_engineering()
        self.exercise_4_few_shot_learning()
        self.exercise_5_text_analysis()
        
        print("\nâœ… ã™ã¹ã¦ã®æ¼”ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("å„æ¼”ç¿’ã®è§£ç­”ã‚’ç¢ºèªã—ã¦ã€ç†è§£ã‚’æ·±ã‚ã¦ãã ã•ã„ã€‚")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("LLMåŸºç¤æ¼”ç¿’å•é¡Œ")
    print("=" * 30)
    
    # æ¼”ç¿’ã‚½ãƒ«ãƒãƒ¼ã®åˆæœŸåŒ–
    solver = LLMExerciseSolver()
    
    # æ¼”ç¿’ã®å®Ÿè¡Œ
    solver.run_all_exercises()

if __name__ == "__main__":
    main()
