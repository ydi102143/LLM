"""
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆGoogle Colabç‰ˆï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€Google Colabç’°å¢ƒã§å®Ÿç”¨çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã—ã¾ã™ã€‚
Colabç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…ã«ãªã£ã¦ã„ã¾ã™ã€‚
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
import numpy as np
import json
import os
from typing import List, Dict, Any, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class ColabChatbotProject:
    """Colabç”¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name="gpt2"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.conversation_history = []
        self.system_prompt = "You are a helpful and friendly AI assistant."
        self.max_history = 5  # Colabç”¨ã«å‰Šæ¸›
        self.max_length = 100  # Colabç”¨ã«å‰Šæ¸›
        self.temperature = 0.7
        self.colab_optimized = True
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("ğŸ”„ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã®èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_parameters():,}")
            print(f"   èªå½™æ•°: {len(self.tokenizer)}")
            
            # Colabç’°å¢ƒã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¡¨ç¤º
            if torch.cuda.is_available():
                print(f"   GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            print("è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ãã ã•ã„")
            return False
    
    def add_to_history(self, role: str, content: str):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        self.conversation_history.append({
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat()
        })
        
        # å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆColabç”¨ã«åˆ¶é™ï¼‰
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def format_conversation(self, user_input: str) -> str:
        """ä¼šè©±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆColabæœ€é©åŒ–ï¼‰"""
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        formatted = f"{self.system_prompt}\\n\\n"
        
        # ä¼šè©±å±¥æ­´ï¼ˆColabç”¨ã«åˆ¶é™ï¼‰
        for message in self.conversation_history[-3:]:  # æœ€æ–°3ä»¶ã®ã¿
            if message['role'] == 'user':
                formatted += f"User: {message['content']}\\n"
            elif message['role'] == 'assistant':
                formatted += f"Assistant: {message['content']}\\n"
        
        # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        formatted += f"User: {user_input}\\nAssistant:"
        
        return formatted
    
    def generate_response(self, user_input: str) -> str:
        """å¿œç­”ã‚’ç”Ÿæˆï¼ˆColabæœ€é©åŒ–ï¼‰"""
        if self.model is None or self.tokenizer is None:
            return "ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        
        try:
            # ä¼šè©±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_conversation = self.format_conversation(user_input)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(formatted_conversation, return_tensors="pt")
            
            # ç”Ÿæˆï¼ˆColabç”¨ã«åˆ¶é™ï¼‰
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + self.max_length,
                    temperature=self.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # å¿œç­”éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            if "Assistant:" in response:
                response = response.split("Assistant:")[-1].strip()
            else:
                response = response[len(formatted_conversation):].strip()
            
            # å¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            response = self.clean_response(response)
            
            return response
            
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def clean_response(self, response: str) -> str:
        """å¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        # ä¸è¦ãªéƒ¨åˆ†ã‚’å‰Šé™¤
        if "User:" in response:
            response = response.split("User:")[0].strip()
        
        # ç©ºã®å¿œç­”ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if not response or len(response.strip()) < 3:
            response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        return response.strip()
    
    def chat(self, user_input: str) -> str:
        """ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å±¥æ­´ã«è¿½åŠ 
        self.add_to_history('user', user_input)
        
        # å¿œç­”ã‚’ç”Ÿæˆ
        response = self.generate_response(user_input)
        
        # å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
        self.add_to_history('assistant', response)
        
        return response
    
    def start_interactive_chat(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nğŸ¤– ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãŒèµ·å‹•ã—ã¾ã—ãŸï¼ï¼ˆColabç‰ˆï¼‰")
        print("=" * 50)
        print("ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã«ã¯ 'clear' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("è¨­å®šã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ 'settings' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ‡ãƒ¢ç”¨ã®ä¼šè©±
        demo_inputs = [
            "ã“ã‚“ã«ã¡ã¯ï¼",
            "ã‚ãªãŸã¯ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ",
            "ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            "ã•ã‚ˆã†ãªã‚‰"
        ]
        
        print("\nğŸ­ ãƒ‡ãƒ¢ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã™")
        for user_input in demo_inputs:
            print(f"\nğŸ‘¤ ã‚ãªãŸ: {user_input}")
            response = self.chat(user_input)
            print(f"ğŸ¤– ãƒœãƒƒãƒˆ: {response}")
        
        print("\nâœ… ãƒ‡ãƒ¢ä¼šè©±ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("å®Ÿéš›ã®ä¼šè©±ã‚’è©¦ã™ã«ã¯ã€ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    def show_settings(self):
        """è¨­å®šã‚’è¡¨ç¤ºãƒ»å¤‰æ›´ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nâš™ï¸ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆè¨­å®šï¼ˆColabç‰ˆï¼‰")
        print("=" * 30)
        print(f"ç¾åœ¨ã®è¨­å®š:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"  æœ€å¤§å±¥æ­´æ•°: {self.max_history}")
        print(f"  æœ€å¤§ç”Ÿæˆé•·: {self.max_length}")
        print(f"  æ¸©åº¦: {self.temperature}")
        print(f"  ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {self.system_prompt}")
        print(f"  Colabæœ€é©åŒ–: {self.colab_optimized}")
        
        print("\nå¤‰æ›´å¯èƒ½ãªè¨­å®š:")
        print("1. æœ€å¤§å±¥æ­´æ•°")
        print("2. æœ€å¤§ç”Ÿæˆé•·")
        print("3. æ¸©åº¦")
        print("4. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
        print("5. æˆ»ã‚‹")
        
        print("\nğŸ’¡ Colabç’°å¢ƒã§ã®åˆ¶é™:")
        print("â€¢ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«ã‚ˆã‚Šã€ä¸€éƒ¨ã®è¨­å®šã¯åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™")
        print("â€¢ å®šæœŸçš„ã«ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢")
        print("â€¢ å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯æ®µéšçš„ã«èª­ã¿è¾¼ã¿")
    
    def export_conversation(self, filename="colab_conversation_history.json"):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print(f"\nğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­: {filename}")
        
        export_data = {
            'model_name': self.model_name,
            'settings': {
                'max_history': self.max_history,
                'max_length': self.max_length,
                'temperature': self.temperature,
                'system_prompt': self.system_prompt,
                'colab_optimized': self.colab_optimized
            },
            'conversation_history': self.conversation_history,
            'export_timestamp': datetime.now().isoformat()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¼šè©±å±¥æ­´ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print("ğŸ’¡ Google Driveã«ä¿å­˜ã™ã‚‹ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„")
    
    def load_conversation(self, filename="colab_conversation_history.json"):
        """ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.conversation_history = data.get('conversation_history', [])
            
            # è¨­å®šã‚’å¾©å…ƒ
            settings = data.get('settings', {})
            self.max_history = settings.get('max_history', self.max_history)
            self.max_length = settings.get('max_length', self.max_length)
            self.temperature = settings.get('temperature', self.temperature)
            self.system_prompt = settings.get('system_prompt', self.system_prompt)
            self.colab_optimized = settings.get('colab_optimized', self.colab_optimized)
            
            print(f"âœ… ä¼šè©±å±¥æ­´ã‚’ {filename} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            print(f"   å±¥æ­´æ•°: {len(self.conversation_history)}ä»¶")
            print(f"   Colabæœ€é©åŒ–: {self.colab_optimized}")
            
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ« {filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            print(f"âŒ èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    def demo_conversation(self):
        """ãƒ‡ãƒ¢ä¼šè©±ã‚’å®Ÿè¡Œï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nğŸ­ ãƒ‡ãƒ¢ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆColabç‰ˆï¼‰")
        print("=" * 40)
        
        demo_inputs = [
            "ã“ã‚“ã«ã¡ã¯ï¼",
            "ã‚ãªãŸã¯ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ",
            "ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            "ã•ã‚ˆã†ãªã‚‰"
        ]
        
        for user_input in demo_inputs:
            print(f"\nğŸ‘¤ ã‚ãªãŸ: {user_input}")
            response = self.chat(user_input)
            print(f"ğŸ¤– ãƒœãƒƒãƒˆ: {response}")
        
        print("\nâœ… ãƒ‡ãƒ¢ä¼šè©±ãŒå®Œäº†ã—ã¾ã—ãŸ")
    
    def run_complete_project(self):
        """å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè¡Œï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("ğŸš€ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é–‹å§‹ã—ã¾ã™ï¼ˆColabç‰ˆï¼‰")
        print("=" * 50)
        
        # 1. ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        if not self.setup_model():
            print("âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’çµ‚äº†ã—ã¾ã™")
            return
        
        # 2. ãƒ‡ãƒ¢ä¼šè©±
        self.demo_conversation()
        
        # 3. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ
        self.start_interactive_chat()
        
        # 4. ä¼šè©±å±¥æ­´ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        self.export_conversation()
        
        print("\nâœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        print("\nğŸ’¡ Colabç’°å¢ƒã§ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹:")
        print("â€¢ å®šæœŸçš„ã«ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢")
        print("â€¢ çµæœã¯Google Driveã«ä¿å­˜")
        print("â€¢ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰è¨­å®šã‚’è»½é‡åŒ–")
        print("â€¢ å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯æ®µéšçš„ã«èª­ã¿è¾¼ã¿")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆGoogle Colabç‰ˆï¼‰")
    print("=" * 40)
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
    chatbot = ColabChatbotProject()
    
    # å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè¡Œ
    chatbot.run_complete_project()

if __name__ == "__main__":
    main()
