"""
æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆGoogle Colabç‰ˆï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€Google Colabç’°å¢ƒã§å®Ÿç”¨çš„ãªæ„Ÿæƒ…åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã—ã¾ã™ã€‚
Colabç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…ã«ãªã£ã¦ã„ã¾ã™ã€‚
"""

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import json
import os
from typing import List, Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

class ColabSentimentAnalysisProject:
    """Colabç”¨æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name="cardiffnlp/twitter-roberta-base-sentiment-latest"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.label_mapping = {0: "Negative", 1: "Neutral", 2: "Positive"}
        self.results = {}
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã®èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model.num_parameters():,}")
            print(f"   èªå½™æ•°: {len(self.tokenizer)}")
            
            # Colabç’°å¢ƒã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¡¨ç¤º
            if torch.cuda.is_available():
                print(f"   GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            print("è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ãã ã•ã„")
            return False
    
    def create_sample_dataset(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆï¼ˆColabç”¨ã«è»½é‡åŒ–ï¼‰"""
        print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
        
        # å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬ã—ãŸã‚µãƒ³ãƒ—ãƒ«ï¼ˆColabç”¨ã«è»½é‡åŒ–ï¼‰
        data = {
            'text': [
                # ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ¬ãƒ“ãƒ¥ãƒ¼
                "I absolutely love this product! It exceeded my expectations and I would definitely buy it again.",
                "Amazing quality and fast delivery. Highly recommend to everyone!",
                "Outstanding customer service and excellent product quality.",
                "This is the best purchase I've made this year. Worth every penny!",
                "Fantastic experience from start to finish. 5 stars!",
                
                # ãƒã‚¬ãƒ†ã‚£ãƒ–ãªãƒ¬ãƒ“ãƒ¥ãƒ¼
                "Terrible product. Complete waste of money and time.",
                "Poor quality and worse customer service. Would not recommend.",
                "Disappointed with this purchase. Not as described at all.",
                "Worst experience ever. Avoid this product at all costs.",
                "Regret buying this. Money down the drain.",
                
                # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãªãƒ¬ãƒ“ãƒ¥ãƒ¼
                "The product is okay. Nothing special but does the job.",
                "Average quality. Expected more for the price.",
                "It's fine, I guess. Not great but not terrible either.",
                "The product works as expected. Nothing more, nothing less.",
                "Decent product but could be better. Middle of the road."
            ],
            'label': [2, 2, 2, 2, 2,  # ãƒã‚¸ãƒ†ã‚£ãƒ–
                     0, 0, 0, 0, 0,  # ãƒã‚¬ãƒ†ã‚£ãƒ–
                     1, 1, 1, 1, 1]  # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«
        }
        
        self.df = pd.DataFrame(data)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(self.df)}ä»¶")
        print(f"   ãƒã‚¸ãƒ†ã‚£ãƒ–: {sum(self.df['label'] == 2)}ä»¶")
        print(f"   ãƒã‚¬ãƒ†ã‚£ãƒ–: {sum(self.df['label'] == 0)}ä»¶")
        print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«: {sum(self.df['label'] == 1)}ä»¶")
        
        return self.df
    
    def analyze_dataset(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ")
        print("=" * 40)
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.df)}")
        print(f"å¹³å‡æ–‡å­—æ•°: {self.df['text'].str.len().mean():.1f}")
        print(f"å¹³å‡å˜èªæ•°: {self.df['text'].str.split().str.len().mean():.1f}")
        
        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
        label_counts = self.df['label'].value_counts().sort_index()
        print("\nãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            sentiment = self.label_mapping[label]
            percentage = count / len(self.df) * 100
            print(f"  {sentiment}: {count}ä»¶ ({percentage:.1f}%)")
        
        # å¯è¦–åŒ–ï¼ˆColabæœ€é©åŒ–ï¼‰
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•
        axes[0].pie(label_counts.values, labels=[self.label_mapping[i] for i in label_counts.index], 
                   autopct='%1.1f%%', startangle=90)
        axes[0].set_title('æ„Ÿæƒ…åˆ†å¸ƒ')
        
        # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ
        text_lengths = self.df['text'].str.len()
        axes[1].hist(text_lengths, bins=10, alpha=0.7, edgecolor='black')
        axes[1].set_xlabel('æ–‡å­—æ•°')
        axes[1].set_ylabel('é »åº¦')
        axes[1].set_title('ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.show()
    
    def predict_sentiment(self, text):
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æï¼ˆColabæœ€é©åŒ–ï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        try:
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=-1).item()
                confidence = predictions[0][predicted_class].item()
            
            return {
                'text': text,
                'predicted_class': predicted_class,
                'sentiment': self.label_mapping[predicted_class],
                'confidence': confidence,
                'all_scores': {
                    self.label_mapping[i]: predictions[0][i].item() 
                    for i in range(len(self.label_mapping))
                }
            }
        except Exception as e:
            return {"error": f"äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"}
    
    def evaluate_model(self, test_data=None):
        """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡")
        print("=" * 40)
        
        if test_data is None:
            test_data = self.df
        
        predictions = []
        confidences = []
        
        for text in test_data['text']:
            result = self.predict_sentiment(text)
            if 'error' not in result:
                predictions.append(result['predicted_class'])
                confidences.append(result['confidence'])
            else:
                print(f"ã‚¨ãƒ©ãƒ¼: {result['error']}")
                return None
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        accuracy = accuracy_score(test_data['label'], predictions)
        f1 = f1_score(test_data['label'], predictions, average='weighted')
        
        print(f"ç²¾åº¦: {accuracy:.3f}")
        print(f"F1ã‚¹ã‚³ã‚¢: {f1:.3f}")
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        report = classification_report(
            test_data['label'], 
            predictions, 
            target_names=list(self.label_mapping.values())
        )
        print(report)
        
        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(test_data['label'], predictions)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=list(self.label_mapping.values()),
                   yticklabels=list(self.label_mapping.values()))
        plt.xlabel('äºˆæ¸¬')
        plt.ylabel('å®Ÿéš›')
        plt.title('æ··åŒè¡Œåˆ—')
        plt.show()
        
        # ä¿¡é ¼åº¦ã®åˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        plt.hist(confidences, bins=10, alpha=0.7, edgecolor='black')
        plt.xlabel('ä¿¡é ¼åº¦')
        plt.ylabel('é »åº¦')
        plt.title('äºˆæ¸¬ã®ä¿¡é ¼åº¦åˆ†å¸ƒ')
        plt.axvline(np.mean(confidences), color='red', linestyle='--', 
                   label=f'å¹³å‡: {np.mean(confidences):.3f}')
        plt.legend()
        plt.show()
        
        self.results = {
            'accuracy': accuracy,
            'f1_score': f1,
            'predictions': predictions,
            'confidences': confidences
        }
        
        return self.results
    
    def interactive_demo(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢ï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("\nğŸ® ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢")
        print("=" * 40)
        print("æ„Ÿæƒ…åˆ†æã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼")
        
        # ãƒ‡ãƒ¢ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
        demo_texts = [
            "I absolutely love this new feature!",
            "This is the worst experience I've ever had.",
            "The product is okay, nothing special.",
            "Amazing quality and excellent service!",
            "I'm not sure how I feel about this."
        ]
        
        print("\nğŸ“Š ãƒ‡ãƒ¢åˆ†æçµæœ:")
        for i, text in enumerate(demo_texts, 1):
            result = self.predict_sentiment(text)
            if 'error' not in result:
                print(f"\n{i}. ãƒ†ã‚­ã‚¹ãƒˆ: {result['text']}")
                print(f"   æ„Ÿæƒ…: {result['sentiment']}")
                print(f"   ä¿¡é ¼åº¦: {result['confidence']:.3f}")
                print(f"   è©³ç´°ã‚¹ã‚³ã‚¢:")
                for sentiment, score in result['all_scores'].items():
                    print(f"     {sentiment}: {score:.3f}")
            else:
                print(f"\n{i}. ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    def batch_analysis(self, texts):
        """ãƒãƒƒãƒåˆ†æï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print(f"\nğŸ“¦ ãƒãƒƒãƒåˆ†æ: {len(texts)}ä»¶")
        print("=" * 40)
        
        results = []
        for i, text in enumerate(texts, 1):
            result = self.predict_sentiment(text)
            results.append(result)
            if 'error' not in result:
                print(f"{i:2d}. {text[:50]}... -> {result['sentiment']} ({result['confidence']:.3f})")
            else:
                print(f"{i:2d}. {text[:50]}... -> ã‚¨ãƒ©ãƒ¼: {result['error']}")
        
        return results
    
    def export_results(self, filename="colab_sentiment_analysis_results.json"):
        """çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print(f"\nğŸ’¾ çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­: {filename}")
        
        export_data = {
            'model_name': self.model_name,
            'label_mapping': self.label_mapping,
            'results': self.results,
            'timestamp': pd.Timestamp.now().isoformat(),
            'colab_optimized': True
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print("ğŸ’¡ Google Driveã«ä¿å­˜ã™ã‚‹ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„")
    
    def run_complete_project(self):
        """å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè¡Œï¼ˆColabæœ€é©åŒ–ï¼‰"""
        print("ğŸš€ æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é–‹å§‹ã—ã¾ã™ï¼ˆColabç‰ˆï¼‰")
        print("=" * 50)
        
        # 1. ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        if not self.setup_model():
            print("âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’çµ‚äº†ã—ã¾ã™")
            return
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
        self.create_sample_dataset()
        
        # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ
        self.analyze_dataset()
        
        # 4. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        self.evaluate_model()
        
        # 5. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢
        self.interactive_demo()
        
        # 6. çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        self.export_results()
        
        print("\nâœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("æ„Ÿæƒ…åˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        print("\nğŸ’¡ Colabç’°å¢ƒã§ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹:")
        print("â€¢ å®šæœŸçš„ã«ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢")
        print("â€¢ çµæœã¯Google Driveã«ä¿å­˜")
        print("â€¢ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰è¨­å®šã‚’è»½é‡åŒ–")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆGoogle Colabç‰ˆï¼‰")
    print("=" * 40)
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
    project = ColabSentimentAnalysisProject()
    
    # å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè¡Œ
    project.run_complete_project()

if __name__ == "__main__":
    main()
